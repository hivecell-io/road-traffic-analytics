## Default values.yaml for Telegraf
## This is a YAML-formatted file.
## ref: https://hub.docker.com/r/library/telegraf/tags/

replicaCount: 1

image:
  repo: "eu.gcr.io/hivecell/helm-demo/telegraf"
  tag: "1.17"
  pullPolicy: IfNotPresent

podAnnotations: {}

podLabels: {}

imagePullSecrets: []

## Configure args passed to Telegraf containers
args: []


# The name of a secret in the same kubernetes namespace which contains values to
# be added to the environment (must be manually created)
# This can be useful for auth tokens, etc.

# envFromSecret: "telegraf-tokens"


env:
  - name: HOSTNAME
    value: "telegraf-polling-service"

# An older "volumeMounts" key was previously added which will likely
# NOT WORK as you expect. Please use this newer configuration.

# volumes:
# - name: telegraf-output-influxdb2
#   configMap:
#     name: "telegraf-output-influxdb2"
# mountPoints:
# - name: telegraf-output-influxdb2
#   mountPath: /etc/telegraf/conf.d
#   subPath: influxdb2.conf


## Configure resource requests and limits
## ref: http://kubernetes.io/docs/user-guide/compute-resources/
resources: {}
  # requests:
  #   memory: 128Mi
  #   cpu: 100m
  # limits:
  #   memory: 128Mi
  #   cpu: 100m

## Node labels for pod assignment
## ref: https://kubernetes.io/docs/user-guide/node-selection/
nodeSelector: {}

## Affinity for pod assignment
## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
##
affinity: {}

## Tolerations for pod assignment
## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
##
tolerations: []
# - key: "key"
#   operator: "Equal|Exists"
#   value: "value"
#   effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"

service:
  enabled: false
  type: ClusterIP
  annotations: {}

rbac:
  # Specifies whether RBAC resources should be created
  create: true
  # Create only for the release namespace or cluster wide (Role vs ClusterRole)
  clusterWide: false
  # Rules for the created rule
  rules: []
# When using the prometheus input to scrape all pods you need extra rules set to the ClusterRole to be
# able to scan the pods for scraping labels. The following rules have been taken from:
# https://github.com/helm/charts/blob/master/stable/prometheus/templates/server-clusterrole.yaml#L8-L46
#    - apiGroups:
#        - ""
#      resources:
#        - nodes
#        - nodes/proxy
#        - nodes/metrics
#        - services
#        - endpoints
#        - pods
#        - ingresses
#        - configmaps
#      verbs:
#        - get
#        - list
#        - watch
#    - apiGroups:
#        - "extensions"
#      resources:
#        - ingresses/status
#        - ingresses
#      verbs:
#        - get
#        - list
#        - watch
#    - nonResourceURLs:
#        - "/metrics"
#      verbs:
#        - get

serviceAccount:
  # Specifies whether a ServiceAccount should be created
  create: true
  # The name of the ServiceAccount to use.
  # If not set and create is true, a name is generated using the fullname template
  name:
  # Annotations for the ServiceAccount
  annotations: {}

## Exposed telegraf configuration
## For full list of possible values see `/docs/all-config-values.yaml` and `/docs/all-config-values.toml`
## ref: https://docs.influxdata.com/telegraf/v1.1/administration/configuration/
config:
    tags:
      dc = "deepstream"
    agent:
      interval = "1s"
      round_interval = false
      metric_batch_size = 500
      metric_buffer_limit = 10000
      collection_jitter = "0s"
      flush_interval = "1s"
      flush_jitter = "0s"
    outputs_influxdb:
      urls = ["http://helm-demo-influxdb.default.svc.cluster.local:8086"]
      database = "grafana"
      username = "admin"
      password = ""
      skip_database_creation = true
    inputs_amqp_consumer:
      brokers = ["amqp://helm-demo-rabbitmq-ha-discovery.default.svc.cluster.local:5672/"]
      exchange = "amq.topic"
      queue = "deepstream"
      username = "guest"
      password = "guest1234A!"
      data_format = "json"
      json_name_key =["object_id","object_vehicle_type","object_vehicle_make","object_vehicle_color"]
      json_string_fields = ["object_id","object_vehicle_type","object_vehicle_make","object_vehicle_color"]
      prefetch_count = 500

metrics:
  health:
    enabled: false
  collect_memstats: false

# Lifecycle hooks
#hooks:
#   postStart: ["/bin/sh", "-c", "sleep 60"]
#   preStop: ["/bin/sh", "-c", "sleep 60"]

## Pod disruption budget configuration
##
pdb:
  ## Specifies whether a Pod disruption budget should be created
  ##
  create: true
  minAvailable: 1
  # maxUnavailable: 1
readinessProbe:
  enabled: true
  initialDelaySeconds: 60
  periodSeconds: 10
  failureThreshold: 40

livenessProbe:
  enabled: true
  initialDelaySeconds: 60
  periodSeconds: 10
  failureThreshold: 40

startupProbe:
   initialDelaySeconds: 40
   periodSeconds: 10
   timeoutSeconds: 10
   successThreshold: 1
   failureThreshold: 2

initContainers:
 command: "sleep 50; until nslookup helm-demo-rabbitmq-ha-discovery.default.svc.cluster.local; do echo waiting for rabbitmq; sleep 50; done"